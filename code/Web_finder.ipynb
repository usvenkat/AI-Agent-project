{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usvenkat/AI-Agent-project/blob/main/code/Web_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVkPbdh79FD6",
        "outputId": "54b19451-d5fe-4e91-cc06-09482c3d9c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install googlesearch-python requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import webbrowser\n",
        "import time\n",
        "from IPython.display import display, HTML\n"
      ],
      "metadata": {
        "id": "QOhq_trZ950a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_topic_info(topic):\n",
        "\n",
        "    important_points = []\n",
        "    time.sleep(2)\n",
        "    print(f\"Searching the web for: {topic}...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        for url in search(topic, num_results=5, lang='en'):\n",
        "            print(f\"Checking general info URL: {url}\")\n",
        "            try:\n",
        "                response = requests.get(url, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                paragraphs = soup.find_all('p')\n",
        "                for p in paragraphs[:3]:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 50:\n",
        "                        important_points.append(text)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Could not access {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "\n",
        "            if len(important_points) > 10:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during web search for general info: {e}\")\n",
        "\n",
        "    return important_points\n",
        "\n"
      ],
      "metadata": {
        "id": "0Y99ZvP_Sif-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_display_image(topic):\n",
        "\n",
        "    print(f\"\\nSearching Google Images for: {topic}...\")\n",
        "    image_search_query = f\"{topic} image\"\n",
        "    direct_image_url = None\n",
        "\n",
        "    try:\n",
        "        for url in search(image_search_query, num_results=5, lang='en', safe='on'):\n",
        "            if \"images.google.com\" in url or \"google.com/images\" in url:\n",
        "                print(f\"Checking image search result URL: {url}\")\n",
        "                try:\n",
        "                    response = requests.get(url, timeout=5)\n",
        "                    response.raise_for_status()\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "                    for meta in soup.find_all('meta', property=\"og:image\"):\n",
        "                        direct_image_url = meta.get('content')\n",
        "                        if direct_image_url and (direct_image_url.startswith('http') or direct_image_url.startswith('https')):\n",
        "                            print(f\"Found potential direct image URL from og:image: {direct_image_url}\")\n",
        "                            break\n",
        "\n",
        "                    if not direct_image_url:\n",
        "\n",
        "                        for img_tag in soup.find_all('img', src=True):\n",
        "                            src = img_tag.get('src')\n",
        "                            if src and (src.startswith('http') or src.startswith('https')) and ('q=tbn' not in src): # Filter out thumbnails\n",
        "                                if any(ext in src for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
        "                                    direct_image_url = src\n",
        "                                    print(f\"Found potential direct image URL from img src: {direct_image_url}\")\n",
        "                                    break\n",
        "                    if direct_image_url:\n",
        "                        break\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Could not access {url} for image: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing {url} for image: {e}\")\n",
        "            if direct_image_url:\n",
        "                break\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Google Images search: {e}\")\n",
        "\n",
        "    if direct_image_url:\n",
        "        print(f\"\\nOpening image in your default web browser: {direct_image_url}\")\n",
        "        try:\n",
        "            webbrowser.open_new_tab(direct_image_url)\n",
        "            return direct_image_url\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open browser: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\nCould not find a direct image URL to display.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "GSyEE7DaSpZt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_info_file(topic, points, image_url=None):\n",
        "    \"\"\"\n",
        "    Creates a text file with the extracted important points and an image URL (if found).\n",
        "    \"\"\"\n",
        "    filename = f\"{topic.replace(' ', '_').lower()}_info.txt\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- Information about: {topic.upper()} ---\\n\\n\")\n",
        "            f.write(\"Important Points:\\n\")\n",
        "            if points:\n",
        "                for i, point in enumerate(points):\n",
        "                    f.write(f\"{i+1}. {point}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"No significant points found.\\n\\n\")\n",
        "\n",
        "            if image_url:\n",
        "                f.write(\"\\n--- Image Displayed (Opened in Browser) ---\\n\")\n",
        "                f.write(f\"The image was opened in your default web browser. You can also view it at:\\n{image_url}\\n\")\n",
        "            else:\n",
        "                f.write(\"No direct image was found or opened in browser.\\n\")\n",
        "\n",
        "        print(f\"\\nInformation saved to: {filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file {filename}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qOUzXvxlSpcQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_info_from_web(query, num_results=3):\n",
        "\n",
        "    print(f\"Searching the web for: {query}\")\n",
        "    extracted_info = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        search_results = search(query, num_results=num_results, lang='en')\n",
        "\n",
        "        for url in search_results:\n",
        "            print(f\"Processing URL: {url}\")\n",
        "            try:\n",
        "\n",
        "                response = requests.get(url, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "                paragraphs = soup.find_all('p')\n",
        "                page_text = \"\"\n",
        "                for p in paragraphs[:5]:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 100:\n",
        "                        page_text += text + \"\\n\\n\"\n",
        "\n",
        "                if page_text:\n",
        "                    extracted_info.append({\"url\": url, \"content\": page_text})\n",
        "                else:\n",
        "                    extracted_info.append({\"url\": url, \"content\": \"No significant text found on this page.\"})\n",
        "\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Could not access {url}: {e}\")\n",
        "                extracted_info.append({\"url\": url, \"content\": f\"Could not access page: {e}\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "                extracted_info.append({\"url\": url, \"content\": f\"Error parsing page: {e}\"})\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during web search: {e}\")\n",
        "\n",
        "    return extracted_info\n"
      ],
      "metadata": {
        "id": "ng0ZxURSX5nn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_google_cse(cx_code=\"23780fbb727f84965\"):\n",
        "\n",
        "  google_cse_code = f\"\"\"\n",
        "  <script async src=\"https://cse.google.com/cse.js?cx={cx_code}\">\n",
        "  </script>\n",
        "  <div class=\"gcse-search\"></div>\n",
        "  \"\"\"\n",
        "\n",
        "  display(HTML(google_cse_code))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic_input = input(\"Enter the topic you want to search for: \")\n",
        "\n",
        "    # Call find_and_display_image BEFORE checking found_image_url\n",
        "    found_image_url = find_and_display_image(topic_input)\n",
        "\n",
        "    points = get_topic_info(topic_input)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Please review the generated text file for information.\")\n",
        "\n",
        "    # Now found_image_url is defined and can be checked\n",
        "    if found_image_url:\n",
        "        time.sleep(2)\n",
        "        print(\"An image related to your topic should have opened in your browser.\")\n",
        "        print()\n",
        "\n",
        "    information = get_info_from_web(topic_input)\n",
        "\n",
        "    print(\"\\n--- Extracted Information ---\")\n",
        "    if information:\n",
        "        for item in information:\n",
        "            print(f\"From URL: {item['url']}\")\n",
        "            print(\"Content:\")\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print(item['content'])\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"No information could be extracted.\")\n",
        "\n",
        "    print(\"Saving file .....\")\n",
        "    # We already found the image URL, so we pass the existing variable\n",
        "    create_info_file(topic_input, points, found_image_url)\n",
        "\n",
        "    print(\"If u wanna search manually \")\n",
        "    time.sleep(2)\n",
        "    print(\"rendering the search block ........\")\n",
        "    time.sleep(2)\n",
        "    print()\n",
        "    print(\"--------------GOOGLE SEARCH----------------\")\n",
        "    print()\n",
        "    display_google_cse()\n",
        "    print()\n",
        "    print(\"-------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZwpYcQ-aSiid",
        "outputId": "df8e23e6-6f33-4cb7-9c38-127681dbcffa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic you want to search for: one piece\n",
            "\n",
            "Searching Google Images for: one piece...\n",
            "\n",
            "Could not find a direct image URL to display.\n",
            "Searching the web for: one piece...\n",
            "Checking general info URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Checking general info URL: /search?num=7\n",
            "Could not access /search?num=7: Invalid URL '/search?num=7': No scheme supplied. Perhaps you meant https:///search?num=7?\n",
            "Checking general info URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Checking general info URL: https://zh.wikipedia.org/zh-tw/ONE_PIECE\n",
            "Checking general info URL: https://onepiece.fandom.com/wiki/One_Piece_Wiki\n",
            "\n",
            "\n",
            "Please review the generated text file for information.\n",
            "Searching the web for: one piece\n",
            "Processing URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Processing URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Processing URL: /search?num=5\n",
            "Could not access /search?num=5: Invalid URL '/search?num=5': No scheme supplied. Perhaps you meant https:///search?num=5?\n",
            "\n",
            "--- Extracted Information ---\n",
            "From URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "One Piece(stylized inall caps) is a Japanesemangaseries written and illustrated byEiichiro Oda. It follows the adventures ofMonkey D. Luffyand his crew, theStraw Hat Pirates, as he explores the Grand Line in search of the mythical treasure known as the \"One Piece\" to become the next King of the Pirates.\n",
            "\n",
            "It has been serialized inShueisha'sshōnenmangamagazineWeekly Shōnen Jumpsince July 1997, with its chapters compiled in 111tankōbonvolumes as of March 2025[update]. The manga series was licensed for an English language release in North America and the United Kingdom byViz Mediaand in Australia byMadman Entertainment. Becoming amedia franchise, it has been adapted into a festival film byProduction I.G, and ananime seriesbyToei Animation, which began broadcasting in 1999. Additionally, Toei has developed 14 animated feature films and oneoriginal video animation. Several companies have developed various types of merchandising and media, such as a trading card game andvideo games.Netflixreleased a live actionTV series adaptation in 2023.\n",
            "\n",
            "It has received praise for the storytelling, world-building, art, characterization, and humour. It has received many awards and is ranked by critics, reviewers, and readers as one of the best manga of all time. By August 2022, it had over 516.6 million copies in circulation in 61 countries and regions worldwide, making it thebest-selling manga seriesin history, and thebest-selling comic seriesprinted in a book volume. Several volumes of the manga have broken publishing records, including the highest initial print run of any book in Japan. In 2015 and 2022,One Pieceset theGuinness World Recordfor \"the most copies published for the same comic book series by a single author\". It was the best-selling manga for 11 consecutive years from 2008 to 2018 and is the only manga that had an initial print of volumes of above 3 million continuously for more than 10 years, as well as the only one that had achieved more than 1 million copies sold in all of its over 100 publishedtankōbonvolumes.One Pieceis the only manga whose volumes have ranked first every year inOricon's weekly comic chart existence since 2008.\n",
            "\n",
            "The Blue Planet ofOne Pieceis populated by humans and other races such asdwarves(more akin tofaeriesin size),giants,merfolk,fish-men, long-limbed tribes, long-necked people known as the Snakeneck Tribe, and animal people (known as \"Minks\"). The Blue Planet is governed by an intercontinental organization known as theWorld Government, consisting of dozens of member countries. TheNavyis the sea military branch of the World Government that protects the known seas from pirates and other criminals. There is alsoCipher Polwhich is a group of agencies within the World Government that are theirsecret police. While pirates are major opponents of the Government, the ones who challenge their rule are theRevolutionary Armywho seek to overthrow them. Thecentral tensionof the series pits the World Government and their forces against pirates. The series regularly emphasizes moral ambiguity over the label \"pirate\", which includes cruel villains, but also any individuals who do not submit to the World Government's authoritarian—and often morally ambiguous—rule. TheOne Pieceworld also has supernormal characteristics like Devil Fruits,[Jp 1]which are mysterious fruits that grant whoever eats them transformative powers at the cost of becoming weakened in bodies of water, resulting in them losing the ability to swim. Another supernatural power is Haki,[Jp 2]which grants its users enhanced willpower, observation, and fighting abilities, and it is one of the only effective methods of inflicting bodily harm on certain Devil Fruit users.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "From URL: https://en.wikipedia.org/wiki/One_Piece\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "One Piece(stylized inall caps) is a Japanesemangaseries written and illustrated byEiichiro Oda. It follows the adventures ofMonkey D. Luffyand his crew, theStraw Hat Pirates, as he explores the Grand Line in search of the mythical treasure known as the \"One Piece\" to become the next King of the Pirates.\n",
            "\n",
            "It has been serialized inShueisha'sshōnenmangamagazineWeekly Shōnen Jumpsince July 1997, with its chapters compiled in 111tankōbonvolumes as of March 2025[update]. The manga series was licensed for an English language release in North America and the United Kingdom byViz Mediaand in Australia byMadman Entertainment. Becoming amedia franchise, it has been adapted into a festival film byProduction I.G, and ananime seriesbyToei Animation, which began broadcasting in 1999. Additionally, Toei has developed 14 animated feature films and oneoriginal video animation. Several companies have developed various types of merchandising and media, such as a trading card game andvideo games.Netflixreleased a live actionTV series adaptation in 2023.\n",
            "\n",
            "It has received praise for the storytelling, world-building, art, characterization, and humour. It has received many awards and is ranked by critics, reviewers, and readers as one of the best manga of all time. By August 2022, it had over 516.6 million copies in circulation in 61 countries and regions worldwide, making it thebest-selling manga seriesin history, and thebest-selling comic seriesprinted in a book volume. Several volumes of the manga have broken publishing records, including the highest initial print run of any book in Japan. In 2015 and 2022,One Pieceset theGuinness World Recordfor \"the most copies published for the same comic book series by a single author\". It was the best-selling manga for 11 consecutive years from 2008 to 2018 and is the only manga that had an initial print of volumes of above 3 million continuously for more than 10 years, as well as the only one that had achieved more than 1 million copies sold in all of its over 100 publishedtankōbonvolumes.One Pieceis the only manga whose volumes have ranked first every year inOricon's weekly comic chart existence since 2008.\n",
            "\n",
            "The Blue Planet ofOne Pieceis populated by humans and other races such asdwarves(more akin tofaeriesin size),giants,merfolk,fish-men, long-limbed tribes, long-necked people known as the Snakeneck Tribe, and animal people (known as \"Minks\"). The Blue Planet is governed by an intercontinental organization known as theWorld Government, consisting of dozens of member countries. TheNavyis the sea military branch of the World Government that protects the known seas from pirates and other criminals. There is alsoCipher Polwhich is a group of agencies within the World Government that are theirsecret police. While pirates are major opponents of the Government, the ones who challenge their rule are theRevolutionary Armywho seek to overthrow them. Thecentral tensionof the series pits the World Government and their forces against pirates. The series regularly emphasizes moral ambiguity over the label \"pirate\", which includes cruel villains, but also any individuals who do not submit to the World Government's authoritarian—and often morally ambiguous—rule. TheOne Pieceworld also has supernormal characteristics like Devil Fruits,[Jp 1]which are mysterious fruits that grant whoever eats them transformative powers at the cost of becoming weakened in bodies of water, resulting in them losing the ability to swim. Another supernatural power is Haki,[Jp 2]which grants its users enhanced willpower, observation, and fighting abilities, and it is one of the only effective methods of inflicting bodily harm on certain Devil Fruit users.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "From URL: /search?num=5\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Could not access page: Invalid URL '/search?num=5': No scheme supplied. Perhaps you meant https:///search?num=5?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Saving file .....\n",
            "\n",
            "Information saved to: one_piece_info.txt\n",
            "If u wanna search manually \n",
            "rendering the search block ........\n",
            "\n",
            "--------------GOOGLE SEARCH----------------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <script async src=\"https://cse.google.com/cse.js?cx=23780fbb727f84965\">\n",
              "  </script>\n",
              "  <div class=\"gcse-search\"></div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}